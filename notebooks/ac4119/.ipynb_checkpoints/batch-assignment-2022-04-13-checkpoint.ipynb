{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 2 (2022-04-13): Selecting the next 140 new examples\n",
    "\n",
    "For each subsequent batch assignment, each rater was given a batch of (n=200), 140 of which are completely new examples, unrated by others. And another 60, that were rated by others. \n",
    "\n",
    "This notebook assigns the batch of new 140 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"/Users/ameliachu/repos/nlu-reddit-toxicity-dataset\"\n",
    "\n",
    "daily_discussion_data_fname = \"daily_master_data_1614250838_1618692612.csv\"\n",
    "daily_discussion_data_path = f\"{repo_dir}/data/{daily_discussion_data_fname}\"\n",
    "\n",
    "randomized_example_ids_path = f\"{repo_dir}/data/randomized_example_ids.p\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in pre-collected dataset and the randomized list of example_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_discussion_data = pd.read_csv(daily_discussion_data_path).rename(columns={'Unnamed: 0':'example_id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_indices = pickle.load( open(randomized_example_ids_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "619646"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(example_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining  raters who need to be assigned a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_ids = ['ac4119', 'gm2858', 'yj2369','yp2201']\n",
    "num_raters = len(rater_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the starting index, and the number of examples to include per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id = 800\n",
    "batch_size = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplmental_batches = []\n",
    "\n",
    "for i in range(num_raters):\n",
    "    end_id = start_id + batch_size\n",
    "    supplmental_batches.append((start_id,end_id))\n",
    "    start_id = end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(800, 940), (940, 1080), (1080, 1220), (1220, 1360)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supplmental_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomizing the order of raters and assigning batches based on order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(rater_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yp2201', (800, 940)), ('gm2858', (940, 1080)), ('ac4119', (1080, 1220)), ('yj2369', (1220, 1360))]\n"
     ]
    }
   ],
   "source": [
    "assign_batches_to_raters =  list(zip(rater_ids,supplmental_batches))\n",
    "print(assign_batches_to_raters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating files for labelling based on batch assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This chunk uses the `rater_id` and assigned indices `(start_ind, end_ind)` as inputs. The process collects the relevant data and generates a file that is more conducive to labelling and text classification training/scoring. Specifically, the below collects the context (i.e. `preceding_comment`, `following_comment`), the `comment_for_evaluation`, and adds columns for each toxic attribute label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_label_dir = f\"{repo_dir}/data/to_label\"\n",
    "current_date = dt.date.today()\n",
    "labels = ['toxicity', 'severe_toxicity', 'identity_attack', 'insult', 'profanity', 'threat']\n",
    "\n",
    "for rater_id, (start_ind, end_ind) in assign_batches_to_raters:\n",
    "    fname = f\"{rater_id}_labelling_assignment_{current_date}\"\n",
    "    export_location = f\"{to_label_dir}/{fname}\"\n",
    "    assigned_indices = example_indices[start_ind:end_ind]\n",
    "    index_map = {}\n",
    "    required_examples = []\n",
    "    for ind in assigned_indices:\n",
    "        ind_examples = [{\n",
    "        'example_id': str(ind),\n",
    "        'example_type': 'preceding',\n",
    "        'body': daily_discussion_data[daily_discussion_data['example_id'] == ind-1]['body'].values[0]\n",
    "    }, {\n",
    "        'example_id': str(ind),\n",
    "        'example_type': 'example',\n",
    "        'body': daily_discussion_data[daily_discussion_data['example_id'] == ind]['body'].values[0]\n",
    "    },\n",
    "     {\n",
    "        'example_id': str(ind),\n",
    "        'example_type': 'following',\n",
    "        'body': daily_discussion_data[daily_discussion_data['example_id'] == ind+1]['body'].values[0]\n",
    "    }]\n",
    "        required_examples += ind_examples \n",
    "    assigned_data = pd.DataFrame(required_examples)\n",
    "    # Adding in the primary example info and the type of example for each row.\n",
    "    assigned_data = assigned_data[['example_type','example_id','body']].reset_index()\n",
    "    # Pivoting the dataframe so that each example_type is its own column\n",
    "    assigned_examples_pivot = assigned_data.pivot(index='example_id', columns='example_type', values='body').reset_index()[['example_id','preceding', 'example','following']]\n",
    "    assigned_examples = assigned_examples_pivot.rename(columns={\n",
    "         'preceding':'preceding_comment',\n",
    "         'following':'following_comment',\n",
    "         'example':'comment_for_evaluation'})\n",
    "    # Adding in columns for each toxic attribute label\n",
    "    for label in labels:\n",
    "         assigned_examples[label] = \"\"\n",
    "    \n",
    "    # Write out file\n",
    "    assigned_examples.to_csv(export_location, index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(assigned_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

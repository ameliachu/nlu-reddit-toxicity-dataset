{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch 4 (2022-04-28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import datetime as dt\n",
    "import pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"/Users/ameliachu/repos/nlu-reddit-toxicity-dataset\"\n",
    "labelled_data_dir = f\"{repo_dir}/data/labelled/\"\n",
    "\n",
    "# Collecting all the labelled file names \n",
    "labelled_data_fnames = [f for f in os.listdir(labelled_data_dir)]\n",
    "\n",
    "# Pre-randomized list of example_ids\n",
    "randomized_example_ids_path = f\"{repo_dir}/data/randomized_example_ids.p\"\n",
    "\n",
    "# example_ids that require backfill due to init assigment issue\n",
    "remaining_backfill_path = f'{repo_dir}/data/backfill_example_ids.p'\n",
    "\n",
    "# Dictionary of example_ids that have been labelled once\n",
    "# See assign-examples-for-interrater.ipynb for more details\n",
    "interrater_assignment_date = '2022-04-21' \n",
    "interrater_assignment_path = f\"{repo_dir}/data/interrater-reliability/interrater_assignment_{interrater_assignment_date}.p\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaining_examples_to_backfill = pickle.load( open(remaining_backfill_path, \"rb\" ) )\n",
    "example_indices = pickle.load( open(randomized_example_ids_path, \"rb\" ) )\n",
    "interrater_assignment = pickle.load( open(interrater_assignment_path, \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaining a list of labelled examples\n",
    "\n",
    "selected_columns = ['example_id']\n",
    "list_of_example_ids = [pd.read_csv(f\"{labelled_data_dir}{fname}\")[selected_columns] for fname in labelled_data_fnames]\n",
    "example_ids_pd = pd.concat(list_of_example_ids)\n",
    "labelled_examples = list(example_ids_pd['example_id'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Defining raters who need to be assigned a batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_ids = ['ac4119', 'gm2858', 'yj2369','yp2201']\n",
    "num_raters = len(rater_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_id = 1360\n",
    "batch_size = 140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_example_indices = example_indices[start_id:]\n",
    "current_example_indices = [i for i in current_example_indices if i not in labelled_examples ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "supplmental_batches = []\n",
    "\n",
    "for i in range(num_raters):\n",
    "    if i == 0:\n",
    "        start_id = 0\n",
    "        n_backfill = len(remaining_examples_to_backfill)\n",
    "        end_id = start_id + batch_size - n_backfill\n",
    "    else:\n",
    "        end_id = start_id + batch_size\n",
    "    supplmental_batches.append((start_id,end_id))\n",
    "    start_id = end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 103), (103, 243), (243, 383), (383, 523)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "supplmental_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomizing the order of raters and assigning batches based on order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(rater_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('yp2201', (1360, 1463)), ('yj2369', (1463, 1603)), ('gm2858', (1603, 1743)), ('ac4119', (1743, 1883))]\n"
     ]
    }
   ],
   "source": [
    "assign_batches_to_raters =  list(zip(rater_ids,supplmental_batches))\n",
    "print(assign_batches_to_raters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading in pre-collected dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_discussion_data_fname = \"daily_master_data_1614250838_1618692612.csv\"\n",
    "daily_discussion_data_path = f\"{repo_dir}/data/{daily_discussion_data_fname}\"\n",
    "daily_discussion_data = pd.read_csv(daily_discussion_data_path).rename(columns={'Unnamed: 0':'example_id'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating files for labelling based on batch assignment\n",
    "\n",
    "This chunk uses the `rater_id` and assigned indices `(start_ind, end_ind)` as inputs. The process collects the relevant data and generates a file that is more conducive to labelling and text classification training/scoring. Specifically, the below collects the context (i.e. `preceding_comment`, `following_comment`), the `comment_for_evaluation`, and adds columns for each toxic attribute label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_label_dir = f\"{repo_dir}/data/to_label\"\n",
    "current_date = dt.date.today()\n",
    "labels = ['toxicity', 'severe_toxicity', 'identity_attack', 'insult', 'profanity', 'threat']\n",
    "\n",
    "for rater_id, (start_ind, end_ind) in assign_batches_to_raters:\n",
    "    fname = f\"{rater_id}_labelling_assignment_{current_date}\"\n",
    "    export_location = f\"{to_label_dir}/{fname}\"\n",
    "    assigned_indices = missing_example_ids[start_ind:end_ind]\n",
    "    assigned_indices += interrater_assignment[rater_id]\n",
    "    index_map = {}\n",
    "    required_examples = []\n",
    "    for ind in assigned_indices:\n",
    "        ind_examples = [{\n",
    "        'example_id': str(ind),\n",
    "        'example_type': 'preceding',\n",
    "        'body': daily_discussion_data[daily_discussion_data['example_id'] == ind-1]['body'].values[0]\n",
    "    }, {\n",
    "        'example_id': str(ind),\n",
    "        'example_type': 'example',\n",
    "        'body': daily_discussion_data[daily_discussion_data['example_id'] == ind]['body'].values[0]\n",
    "    },\n",
    "     {\n",
    "        'example_id': str(ind),\n",
    "        'example_type': 'following',\n",
    "        'body': daily_discussion_data[daily_discussion_data['example_id'] == ind+1]['body'].values[0]\n",
    "    }]\n",
    "        required_examples += ind_examples \n",
    "    assigned_data = pd.DataFrame(required_examples)\n",
    "    # Adding in the primary example info and the type of example for each row.\n",
    "    assigned_data = assigned_data[['example_type','example_id','body']].reset_index()\n",
    "    assigned_examples_pivot = assigned_data.pivot(index='example_id', columns='example_type', values='body').reset_index()[['example_id','preceding', 'example','following']]\n",
    "    assigned_examples = assigned_examples_pivot.rename(columns={\n",
    "         'preceding':'preceding_comment',\n",
    "         'following':'following_comment',\n",
    "         'example':'comment_for_evaluation'})\n",
    "    for label in labels:\n",
    "         assigned_examples[label] = \"\"\n",
    "    assigned_examples.to_csv(export_location, index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

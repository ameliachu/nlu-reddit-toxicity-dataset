{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"/Users/ameliachu/repos/nlu-reddit-toxicity-dataset\"\n",
    "labelled_data_dir = f\"{repo_dir}/data/labelled/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yj2369_labelling_assignment_2022-04-13.csv',\n",
       " 'gm2858_labelling_assignment_2022-04-09.csv',\n",
       " 'yp2201_labelling_assignment_2022-04-09.csv',\n",
       " 'ac4119_labelling_assignment_2022-04-09.csv',\n",
       " 'ac4119_labelling_assignment_2022-04-13.csv',\n",
       " 'yp2201_labelling_assignment_2022-04-13.csv',\n",
       " 'gm2858_labelling_assignment_2022-04-13.csv',\n",
       " 'yj2369_labelling_assignment_2022-04-09.csv']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data_fnames = os.listdir(labelled_data_dir)\n",
    "labelled_data_fnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxicity', 'severe_toxicity', 'identity_attack', 'insult', 'profanity', 'threat']\n",
    "list_of_labelled_examples = []\n",
    "\n",
    "for fname in labelled_data_fnames:\n",
    "   \n",
    "    labelled_data = pd.read_csv(f\"{labelled_data_dir}{fname}\")\n",
    "    labelled_data['has_toxicity'] = labelled_data[labels].sum(axis=1)\n",
    "    labelled_data['has_toxicity'] = labelled_data['has_toxicity'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    labelled_data['rater_id'] = fname.split(\"_\")[0]\n",
    "    labelled_data['assignment_date'] = fname.split(\"_\")[-1][:-4]\n",
    "    example_id_df = labelled_data[['assignment_date','example_id','has_toxicity','rater_id']]\n",
    "    list_of_labelled_examples.append(example_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_examples_lookup = pd.concat(list_of_labelled_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_raters_lookup = labelled_examples_lookup.groupby(\"example_id\").rater_id.nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_raters_lookup.columns = ['example_id', 'n_raters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_examples_lookup_2 = pd.merge(labelled_examples_lookup, n_raters_lookup, on=['example_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1361"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labelled_examples_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_examples = labelled_examples_lookup_2['has_toxicity'] == 1\n",
    "nontoxic_examples = labelled_examples_lookup_2['has_toxicity'] == 0\n",
    "need_interrater = labelled_examples_lookup_2['n_raters'] <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "rater_ids = ['ac4119', 'gm2858', 'yj2369','yp2201']\n",
    "n_raters = len(rater_ids)\n",
    "n_other_raters = n_raters - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ratio = {\n",
    "    'nontoxic': 26,\n",
    "    'toxic': 14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac4119 99 242\n",
      "gm2858 54 285\n",
      "yj2369 61 277\n",
      "yp2201 44 295\n"
     ]
    }
   ],
   "source": [
    "toxic_df = labelled_examples_lookup_2[toxic_examples & need_interrater]\n",
    "nontoxic_df = labelled_examples_lookup_2[nontoxic_examples & need_interrater]\n",
    "selected_examples = {}\n",
    "\n",
    "for rater in rater_ids:\n",
    "    toxic = toxic_df[toxic_df['rater_id'] == rater]['example_id']\n",
    "    nontoxic = nontoxic_df[nontoxic_df['rater_id'] == rater]['example_id']\n",
    "    print(rater, len(toxic),len(nontoxic))\n",
    "    \n",
    "    toxic = toxic.sample(sample_ratio['toxic'] * (n_raters-1))\n",
    "    nontoxic = nontoxic.sample(sample_ratio['nontoxic'] * (n_raters-1))\n",
    "    \n",
    "    selected_examples[rater] = {\n",
    "        'toxic': list(toxic.values),\n",
    "        'nontoxic':  list(nontoxic.values),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = (n_raters - 1) * 2 # 2 = has_toxicity TRUE, FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = {\n",
    "    \"toxic\": [],\n",
    "    \"nontoxic\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "toxic_start_id = 0\n",
    "nontoxic_start_id = 0\n",
    "\n",
    "for i in range(n_other_raters):\n",
    "    toxic_end_id = toxic_start_id + sample_ratio['toxic']\n",
    "    nontoxic_end_id = nontoxic_start_id + sample_ratio['nontoxic']\n",
    "    batch_indices['toxic'].append((toxic_start_id, toxic_end_id))\n",
    "    batch_indices['nontoxic'].append((nontoxic_start_id, nontoxic_end_id))\n",
    "    toxic_start_id = toxic_end_id\n",
    "    nontoxic_start_id = nontoxic_end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': [(0, 14), (14, 28), (28, 42)],\n",
       " 'nontoxic': [(0, 26), (26, 52), (52, 78)]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic assignment order: [('yp2201', (0, 14)), ('yj2369', (14, 28)), ('gm2858', (28, 42))]\n",
      "nontoxic assignment order: [('gm2858', (0, 26)), ('yj2369', (26, 52)), ('yp2201', (52, 78))]\n",
      "toxic assignment order: [('yp2201', (0, 14)), ('yj2369', (14, 28)), ('ac4119', (28, 42))]\n",
      "nontoxic assignment order: [('yj2369', (0, 26)), ('ac4119', (26, 52)), ('yp2201', (52, 78))]\n",
      "toxic assignment order: [('gm2858', (0, 14)), ('ac4119', (14, 28)), ('yp2201', (28, 42))]\n",
      "nontoxic assignment order: [('yp2201', (0, 26)), ('ac4119', (26, 52)), ('gm2858', (52, 78))]\n",
      "toxic assignment order: [('gm2858', (0, 14)), ('ac4119', (14, 28)), ('yj2369', (28, 42))]\n",
      "nontoxic assignment order: [('gm2858', (0, 26)), ('yj2369', (26, 52)), ('ac4119', (52, 78))]\n"
     ]
    }
   ],
   "source": [
    "interrater_assignment = {r:[] for r in rater_ids}\n",
    "\n",
    "for rater in rater_ids:\n",
    "    other_raters = [r for r in rater_ids if r!=rater]\n",
    "    for example_type in ['toxic','nontoxic']:\n",
    "        example_list = selected_examples[rater][example_type]\n",
    "        sample_size = len(example_list)\n",
    "        sample_ranges = batch_indices[example_type]\n",
    "        random.shuffle(other_raters)\n",
    "        assign_batches_to_raters = list(zip(other_raters, sample_ranges))\n",
    "        print(f'{example_type} assignment order:', assign_batches_to_raters)\n",
    "        for other_rater, (start_id, end_id) in assign_batches_to_raters:\n",
    "            interrater_assignment[other_rater] += selected_examples[rater][example_type][start_id:end_id] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ameliachu/repos/nlu-reddit-toxicity-dataset/data/interrater-reliability/interrater_assignment_2022-04-21.p\n"
     ]
    }
   ],
   "source": [
    "interrater_assignment_path =  f\"{repo_dir}/data/interrater-reliability/interrater_assignment_{dt.date.today()}.p\"\n",
    "print(interrater_assignment_path )\n",
    "pickle.dump(interrater_assignment, open( interrater_assignment_path, \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

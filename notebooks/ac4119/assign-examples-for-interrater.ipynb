{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting `example_ids` to enable Interrater Reliability Checking\n",
    "This notebook is run after all data from a batch is labelled. It identifies `example_ids` that have been labeled by only one other rater so that they can be randomly selected to be included in the subsequent batch for interrater reliability checking.\n",
    "\n",
    "We want to ensure that we are prioritizing examples that contain toxicity, and that each rater is receiving an equal amount of examples from each other rater.\n",
    "\n",
    "To illustrate, if each batch contains 60 examples that have been previously rated once:\n",
    "- The rater A will receive 20 examples each from raters B, C, D.\n",
    "- Of these 20 examples, more than half of the examples contain at least one toxic attribute.\n",
    "\n",
    "The resulting pickle file will contain the randomly assigned `example_ids` for each rater and be read in by the next batch assignment notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import random\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_dir = \"/Users/ameliachu/repos/nlu-reddit-toxicity-dataset\"\n",
    "labelled_data_dir = f\"{repo_dir}/data/labelled/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gathering file names of all labelled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yj2369_labelling_assignment_2022-04-13.csv',\n",
       " 'gm2858_labelling_assignment_2022-04-09.csv',\n",
       " 'yp2201_labelling_assignment_2022-04-09.csv',\n",
       " 'ac4119_labelling_assignment_2022-04-09.csv',\n",
       " 'ac4119_labelling_assignment_2022-04-13.csv',\n",
       " 'yp2201_labelling_assignment_2022-04-13.csv',\n",
       " 'gm2858_labelling_assignment_2022-04-13.csv',\n",
       " 'yj2369_labelling_assignment_2022-04-09.csv']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelled_data_fnames = os.listdir(labelled_data_dir)\n",
    "labelled_data_fnames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placing all labelled data to-date in a dataframe and appending additional metadata needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['toxicity', 'severe_toxicity', 'identity_attack', 'insult', 'profanity', 'threat']\n",
    "list_of_labelled_examples = []\n",
    "\n",
    "for fname in labelled_data_fnames:\n",
    "   \n",
    "    labelled_data = pd.read_csv(f\"{labelled_data_dir}{fname}\")\n",
    "    # Creating a summary column 'has_toxicity' to flag examples \n",
    "    # which contain at least one toxic attribute\n",
    "    labelled_data['has_toxicity'] = labelled_data[labels].sum(axis=1)\n",
    "    labelled_data['has_toxicity'] = labelled_data['has_toxicity'].apply(lambda x: 1 if x > 0 else 0)\n",
    "    labelled_data['rater_id'] = fname.split(\"_\")[0]\n",
    "    labelled_data['assignment_date'] = fname.split(\"_\")[-1][:-4]\n",
    "    example_id_df = labelled_data[['assignment_date','example_id','has_toxicity','rater_id']]\n",
    "    list_of_labelled_examples.append(example_id_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_examples_lookup = pd.concat(list_of_labelled_examples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating `n_raters_lookup` to determine if each example has already had an interater check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_raters_lookup = labelled_examples_lookup.groupby(\"example_id\").rater_id.nunique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_raters_lookup.columns = ['example_id', 'n_raters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_examples_lookup_2 = pd.merge(labelled_examples_lookup, n_raters_lookup, on=['example_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating `selected_examples`, a dictionary that contains n randomly-selected `example_ids` of each `rater_id` x `has_toxicity` combination, based on the pre-defined `sample_ratio`.\n",
    "\n",
    "`selected_examples` will be used to assign each rater examples for interrater reliability checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating pandas masks to simplify filtering\n",
    "toxic_examples = labelled_examples_lookup_2['has_toxicity'] == 1\n",
    "nontoxic_examples = labelled_examples_lookup_2['has_toxicity'] == 0\n",
    "need_interrater = labelled_examples_lookup_2['n_raters'] <= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the raters\n",
    "rater_ids = ['ac4119', 'gm2858', 'yj2369','yp2201']\n",
    "n_raters = len(rater_ids)\n",
    "n_other_raters = n_raters - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the sample ratio of examples with toxic attributes vs. samples without any.\n",
    "sample_ratio = {\n",
    "    'nontoxic': 26,\n",
    "    'toxic': 14\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ac4119 99 242\n",
      "gm2858 54 285\n",
      "yj2369 61 277\n",
      "yp2201 44 295\n"
     ]
    }
   ],
   "source": [
    "toxic_df = labelled_examples_lookup_2[toxic_examples & need_interrater]\n",
    "nontoxic_df = labelled_examples_lookup_2[nontoxic_examples & need_interrater]\n",
    "selected_examples = {}\n",
    "\n",
    "for rater in rater_ids:\n",
    "    toxic = toxic_df[toxic_df['rater_id'] == rater]['example_id']\n",
    "    nontoxic = nontoxic_df[nontoxic_df['rater_id'] == rater]['example_id']\n",
    "    print(rater, len(toxic),len(nontoxic))\n",
    "    \n",
    "    toxic = toxic.sample(sample_ratio['toxic'] * (n_raters-1))\n",
    "    nontoxic = nontoxic.sample(sample_ratio['nontoxic'] * (n_raters-1))\n",
    "    \n",
    "    selected_examples[rater] = {\n",
    "        'toxic': list(toxic.values),\n",
    "        'nontoxic':  list(nontoxic.values),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the batch indices for \"toxic\" and \"nontoxic\" examples\n",
    "This calculates the indices that should be assigned based on the total number of examples of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_batches = (n_raters - 1) * 2 # 2 = has_toxicity TRUE, FALSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = {\n",
    "    \"toxic\": [],\n",
    "    \"nontoxic\": []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "toxic_start_id = 0\n",
    "nontoxic_start_id = 0\n",
    "\n",
    "for i in range(n_other_raters):\n",
    "    toxic_end_id = toxic_start_id + sample_ratio['toxic']\n",
    "    nontoxic_end_id = nontoxic_start_id + sample_ratio['nontoxic']\n",
    "    batch_indices['toxic'].append((toxic_start_id, toxic_end_id))\n",
    "    batch_indices['nontoxic'].append((nontoxic_start_id, nontoxic_end_id))\n",
    "    toxic_start_id = toxic_end_id\n",
    "    nontoxic_start_id = nontoxic_end_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'toxic': [(0, 14), (14, 28), (28, 42)],\n",
       " 'nontoxic': [(0, 26), (26, 52), (52, 78)]}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining batch assignment for each rater\n",
    "\n",
    "Each rater will receive an equal amount of examples from each other rater, at the same toxic vs. non-toxic ratio. This determines how each labelled `rater_id` x `has_toxicity` subset should be split and randomly assigned to each `other_rater`. Then, it applies those and saves those assignments, producing `interrater_assignment`.\n",
    "```\n",
    "interrater_assignment = Dict(rater_id,list_of_example_ids)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic assignment order: [('yp2201', (0, 14)), ('yj2369', (14, 28)), ('gm2858', (28, 42))]\n",
      "nontoxic assignment order: [('gm2858', (0, 26)), ('yj2369', (26, 52)), ('yp2201', (52, 78))]\n",
      "toxic assignment order: [('yp2201', (0, 14)), ('yj2369', (14, 28)), ('ac4119', (28, 42))]\n",
      "nontoxic assignment order: [('yj2369', (0, 26)), ('ac4119', (26, 52)), ('yp2201', (52, 78))]\n",
      "toxic assignment order: [('gm2858', (0, 14)), ('ac4119', (14, 28)), ('yp2201', (28, 42))]\n",
      "nontoxic assignment order: [('yp2201', (0, 26)), ('ac4119', (26, 52)), ('gm2858', (52, 78))]\n",
      "toxic assignment order: [('gm2858', (0, 14)), ('ac4119', (14, 28)), ('yj2369', (28, 42))]\n",
      "nontoxic assignment order: [('gm2858', (0, 26)), ('yj2369', (26, 52)), ('ac4119', (52, 78))]\n"
     ]
    }
   ],
   "source": [
    "interrater_assignment = {r:[] for r in rater_ids}\n",
    "\n",
    "for rater in rater_ids:\n",
    "    other_raters = [r for r in rater_ids if r!=rater]\n",
    "    for example_type in ['toxic','nontoxic']:\n",
    "        example_list = selected_examples[rater][example_type]\n",
    "        sample_size = len(example_list)\n",
    "        sample_ranges = batch_indices[example_type]\n",
    "        random.shuffle(other_raters)\n",
    "        assign_batches_to_raters = list(zip(other_raters, sample_ranges))\n",
    "        print(f'{example_type} assignment order:', assign_batches_to_raters)\n",
    "        for other_rater, (start_id, end_id) in assign_batches_to_raters:\n",
    "            interrater_assignment[other_rater] += selected_examples[rater][example_type][start_id:end_id] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving assignment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/ameliachu/repos/nlu-reddit-toxicity-dataset/data/interrater-reliability/interrater_assignment_2022-04-21.p\n"
     ]
    }
   ],
   "source": [
    "interrater_assignment_path =  f\"{repo_dir}/data/interrater-reliability/interrater_assignment_{dt.date.today()}.p\"\n",
    "print(interrater_assignment_path )\n",
    "pickle.dump(interrater_assignment, open( interrater_assignment_path, \"wb\" ) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
